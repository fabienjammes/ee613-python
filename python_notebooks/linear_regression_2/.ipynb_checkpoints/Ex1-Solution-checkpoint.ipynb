{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE613 - Linear Regression II - Exercise 1 (Logistic Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv\n",
    "from numpy import dot\n",
    "\n",
    "#use 'inline' for plotting the figure inside the notebook, and 'qt' for pop-up plot\n",
    "%matplotlib inline\n",
    "\n",
    "np.set_printoptions(precision=4,suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this exercise, we want to apply logistic regression to classify between two letters, $C_1$ and $C_2$, given the data $x$. The probability of belonging to the first class $C_1$ can be formulated as: \n",
    "$$P(C_1 | x) = \\frac{1}{1 + exp(-A^\\top x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '../python_data/2Dletters/'\n",
    "\n",
    "#take the first letter data\n",
    "data = np.load(ROOT + 'U.npy')\n",
    "data = data.transpose([0,2,1])\n",
    "data = data[:,::5,:]\n",
    "num_data_1 = len(data)\n",
    "\n",
    "#take the second letter data\n",
    "data2 = np.load(ROOT + 'V.npy')\n",
    "data2 = data2.transpose([0,2,1])\n",
    "data2 = data2[:,::5,:]\n",
    "num_data_2 = len(data2)\n",
    "\n",
    "#combine the data\n",
    "data_full = np.concatenate((data,data2),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot random sample from the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3b435aba50>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_index = np.random.randint(len(data_full))\n",
    "data_i = data_full[random_index]\n",
    "plt.plot(data_i[:,0], data_i[:,1],'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the input and output, and separate into training and test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#the input is the letter trajectory data {x1,x2}, and the output is a binary variable {0,1}\n",
    "N = data_full.shape[0]\n",
    "data_full = data_full.reshape(N,-1) #concatenating x1 and x2 variables into a vector\n",
    "X = np.concatenate([np.ones((N,1)), data_full.reshape(N,-1)], axis=1)\n",
    "Y = np.concatenate([np.ones(num_data_1),np.zeros(num_data_2)])[:,None] #the output value for the first letter is 1, and 0 for the second letter\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: \n",
    "Using $X_{train}$ and $Y_{train}$, obtain the logistic regression parameters $A$ by IRLS, and use that to predict the correct letters on $X_{test}$. Compare the prediction to the true value, $Y_{test}$, at different iterations of IRLS. You can look at demo_logistic_regression.ipynb for an example. (Note: chose an array of zeros as the initial value of $A$).   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2:\n",
    "When constructing the input $X$ above, why do we need to add np.ones((N,1))?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteratively Reweighted Least Squares (IRLS) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_likelihood(y_pred, y_true):\n",
    "    N = len(y_pred)\n",
    "    L = 0\n",
    "    for n in range(N):\n",
    "        L -= y_true[n,0]*np.log(y_pred[n,0]+1e-7) + (1-y_true[n,0])*np.log(1-y_pred[n,0]+1e-7)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X,Y, X_test=None, Y_test = None, nb_iter = 5, lamda = 0.1):\n",
    "    n_in = X.shape[1]\n",
    "    a = np.zeros((n_in,1))\n",
    "    res = []\n",
    "    L_prev = np.inf\n",
    "    for i in range(nb_iter):\n",
    "        mu = 1./(1 + np.exp(-dot(X,a))) #prediction\n",
    "        W = np.diag((mu * (1-mu)).flatten())\n",
    "        a = dot(inv(dot(X.T,dot(W,X))+1e-7*np.eye(n_in)),dot(X.T, dot(W,dot(X,a))+(Y-mu)))\n",
    "        \n",
    "        if Y_test is not None:\n",
    "            Y_pred = 1./(1 + np.exp(-dot(X_test,a)))\n",
    "            res += [np.linalg.norm(Y_pred-Y_test)]\n",
    "\n",
    "    return a, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(X,Y, X_test=None, Y_test = None, nb_iter = 5, lamda = 0.1):\n",
    "    n_in = X.shape[1]\n",
    "    a = np.random.rand(n_in,1)\n",
    "    #a = np.zeros((n_in,1))\n",
    "    res = []\n",
    "    log_probs = []\n",
    "    L_prev = np.inf\n",
    "    for i in range(nb_iter):\n",
    "        mu = 1./(1 + np.exp(-dot(X,a))) #prediction\n",
    "        W = np.diag((mu * (1-mu)).flatten())\n",
    "        \n",
    "        \n",
    "        #find the step length\n",
    "        lamda = 1\n",
    "        L = np.inf\n",
    "        while L >= L_prev:\n",
    "            a_new = a - lamda*dot(inv(dot(X.T,dot(W,X))+1e-7*np.eye(n_in)),dot(X.T, mu - Y))\n",
    "            #a_new = dot(inv(dot(X.T,dot(W,X))+1e-11*np.eye(n_in)),dot(X.T, dot(W,dot(X,a))+lamda*(Y-mu)))\n",
    "            #print a_new.flatten()\n",
    "            mu_new = 1./(1 + np.exp(-dot(X,a_new))) #prediction\n",
    "            L = compute_log_likelihood(mu_new, Y)-1e-4\n",
    "            if L >= L_prev:\n",
    "                lamda*= 0.2\n",
    "                print 'reduce lamda to' + str(lamda)\n",
    "                #print 'A difference: {}'.format(a-a_new)\n",
    "            print '{},{}'.format(L_prev, L)\n",
    "                \n",
    "        print 'good lamda is:' + str(lamda)\n",
    "        a = a_new.copy()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if Y_test is not None:\n",
    "            Y_pred = 1./(1 + np.exp(-dot(X_test,a)))\n",
    "            res += [np.linalg.norm(Y_pred-Y_test)]\n",
    "        #calculate the log probability\n",
    "        #L = compute_log_likelihood(mu, Y)\n",
    "        log_probs += [L]\n",
    "        L_prev = L\n",
    "    return a, res, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46316144845419754, 0.30556423717461617, 0.22711115801669762, 0.17398427815422074, 0.13358319700732965, 0.10200595387504895, 0.077371105599755269, 0.058319312531923108, 0.043702418858933986, 0.032525394805847319]\n"
     ]
    }
   ],
   "source": [
    "A,res = logistic_regression(X_train, Y_train, X_test, Y_test, nb_iter = 10)\n",
    "\n",
    "print res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_gd(X,Y,X_test=None, Y_test=None, nb_iter = 5, lamda=1):\n",
    "    n_in = X.shape[1]\n",
    "    #a = np.random.rand(n_in,1)\n",
    "    a = np.zeros((n_in,1))\n",
    "    res = []\n",
    "    log_probs = []\n",
    "    for i in range(nb_iter):\n",
    "        y = 1./(1 + np.exp(-dot(X,a)))\n",
    "        grad = dot(X.T, y - Y)\n",
    "        a = a - lamda*grad\n",
    "        #calculate res\n",
    "        if Y_test is not None:\n",
    "            y_pred = 1./(1 + np.exp(-dot(X_test,a)))\n",
    "            res += [np.linalg.norm(y_pred-Y_test)]\n",
    "        #calculate the log probability\n",
    "        L = compute_log_likelihood(mu, Y)\n",
    "        log_probs += [L]\n",
    "        print L\n",
    "            \n",
    "    return a,res,log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.4766456501\n",
      "11.5082937666\n",
      "10.711122825\n",
      "9.9990409531\n",
      "9.35996868111\n",
      "8.78518640987\n",
      "8.26699265496\n",
      "7.79860408071\n",
      "7.37408112077\n",
      "6.98824414601\n",
      "[2.0053567817471798, 1.9043507467852303, 1.8108396868877319, 1.7236480535357286, 1.6426768592914514, 1.5675414725490995, 1.4978594906096863, 1.4332276939102799, 1.3732476530957802, 1.3175361469160833]\n",
      "[ 0.7111  0.1916  0.3667  0.2507  0.5748  0.5235  0.2572]\n"
     ]
    }
   ],
   "source": [
    "A,res,log_probs = logistic_regression_gd(X_train, Y_train, X_train, Y_train, nb_iter = 10,lamda=0.0001)\n",
    "\n",
    "print res\n",
    "Y_pred = 1./(1 + np.exp(-dot(X_test,A)))\n",
    "print Y_pred.flatten()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
